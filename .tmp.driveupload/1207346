# üîß Installation Guide - v4.1.0 OSS Edition

Get Wag-Tail AI Gateway v4.1.0 OSS Edition running in 5 minutes with this comprehensive installation guide.

## üìã Prerequisites

### System Requirements
- **Python 3.9+** (Python 3.11+ recommended)
- **2GB+ RAM** (4GB+ recommended for production)
- **1GB+ disk space**
- **Internet connection** (for LLM provider APIs)

### Supported Operating Systems
- ‚úÖ **macOS** (Intel & Apple Silicon)
- ‚úÖ **Linux** (Ubuntu 20.04+, CentOS 8+, Debian 11+)
- ‚úÖ **Windows** (Windows 10+, WSL2 recommended)

## üöÄ Quick Installation

### Method 1: Standard Installation

```bash
# 1. Clone the repository
git clone https://github.com/startoken-wag-tail/wag-tail-ai-gateway.git
cd wag-tail-ai-gateway

# 2. Create virtual environment
python3 -m venv venv

# 3. Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
# venv\Scripts\activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Start the gateway
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Method 2: Development Installation

```bash
# Clone and setup for development
git clone https://github.com/startoken-wag-tail/wag-tail-ai-gateway.git
cd wag-tail-ai-gateway

# Create development environment
python3 -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install with development dependencies
pip install -r requirements.txt
pip install pytest black flake8  # Development tools

# Run tests to verify installation
python total_test_OSS.py
```

## ‚öôÔ∏è LLM Provider Setup

### Option 1: Ollama (Local LLMs - Recommended)

```bash
# Install Ollama
# macOS:
brew install ollama

# Linux:
curl -fsSL https://ollama.ai/install.sh | sh

# Windows: Download from https://ollama.ai

# Pull a model
ollama pull mistral
ollama serve  # Starts on localhost:11434
```

**Configuration:**
```yaml
# config/sys_config.yaml
llm:
  provider: "ollama"
  model: "mistral"
  api_url: "http://localhost:11434/api/generate"
  api_key: ""  # Leave empty for Ollama
```

### Option 2: OpenAI

```bash
# Set environment variable
export OPENAI_API_KEY="sk-your-key-here"
```

**Configuration:**
```yaml
# config/sys_config.yaml
llm:
  provider: "openai"
  model: "gpt-3.5-turbo"
  api_key: "${OPENAI_API_KEY}"
```

### Option 3: Google Gemini

```bash
# Set environment variable
export GEMINI_API_KEY="your-gemini-key-here"
```

**Configuration:**
```yaml
# config/sys_config.yaml
llm:
  provider: "gemini"
  model: "gemini-2.0-flash"
  api_key: "${GEMINI_API_KEY}"
```

### Option 4: Azure OpenAI

```bash
# Set environment variables
export AZURE_OPENAI_API_KEY="your-azure-key"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
```

**Configuration:**
```yaml
# config/sys_config.yaml
llm:
  provider: "azure"
  model: "gpt-35-turbo"
  api_key: "${AZURE_OPENAI_API_KEY}"
  api_url: "${AZURE_OPENAI_ENDPOINT}"
```

## ‚úÖ Verification

### 1. Health Check
```bash
curl http://localhost:8000/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "version": "4.3.0",
  "edition": "oss",
  "llm_status": "available",
  "plugins_loaded": 3
}
```

### 2. Test Chat Request
```bash
curl -X POST http://localhost:8000/chat \
  -H "X-API-Key: demo-key-for-testing" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is 2+2?"}'
```

### 3. Run Full Test Suite
```bash
python total_test_OSS.py
```

**Expected**: 50/50 tests passing

## üê≥ Docker Installation (Coming Soon)

```bash
# Quick Docker setup (future release)
docker run -p 8000:8000 startoken/wag-tail-gateway:latest
```

## üîß Configuration Files

After installation, configure these files:

| File | Purpose | Required |
|------|---------|----------|
| `config/sys_config.yaml` | Main system configuration | ‚úÖ Yes |
| `config/environments/development.yaml` | Development overrides | Optional |
| `config/environments/production.yaml` | Production overrides | Optional |

## üö® Troubleshooting

### Common Issues

**Issue: "Module not found" errors**
```bash
# Solution: Ensure virtual environment is activated
source venv/bin/activate  # macOS/Linux
# or
venv\Scripts\activate     # Windows
```

**Issue: "Port already in use"**
```bash
# Solution: Use different port
uvicorn main:app --port 8001
```

**Issue: "LLM provider not available"**
```bash
# Solution: Check provider configuration and API keys
curl http://localhost:8000/health
```

**Issue: Presidio warnings**
```bash
# Solution: Install Presidio for PII detection (optional)
pip install presidio-analyzer presidio-anonymizer
```

### Performance Issues

**Slow startup:**
- Ensure adequate RAM (4GB+ recommended)
- Check disk space availability
- Verify network connectivity to LLM providers

**High memory usage:**
- Reduce concurrent requests
- Consider using Ollama for local processing
- Monitor system resources

## üìû Getting Help

- **üìö Documentation**: Continue with [Configuration](Configuration)
- **üí¨ Community**: [GitHub Discussions](../../discussions)
- **üêõ Issues**: [Report bugs](../../issues)
- **üìß Support**: For enterprise support needs

## ‚û°Ô∏è Next Steps

1. ‚úÖ Installation complete
2. ‚öôÔ∏è **Configure your LLM provider** ‚Üí [Configuration Guide](Configuration)
3. üõ°Ô∏è **Setup security features** ‚Üí [Security Features](Security-Features)
4. üöÄ **Deploy to production** ‚Üí [Deployment Guide](Deployment)

---

**üéâ Congratulations! Wag-Tail AI Gateway v4.1.0 OSS Edition is now installed and ready to secure your AI applications.**